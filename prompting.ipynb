{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ######################## PART 1: PROVIDED CODE ########################\n",
    "\n",
    "\n",
    "def load_datasets(data_directory: str) -> Union[dict, dict]:\n",
    "    \"\"\"\n",
    "    Reads the training and validation splits from disk and load\n",
    "    them into memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_directory: str\n",
    "        The directory where the data is stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train: dict\n",
    "        The train dictionary with keys 'premise', 'hypothesis', 'label'.\n",
    "    validation: dict\n",
    "        The validation dictionary with keys 'premise', 'hypothesis', 'label'.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    with open(os.path.join(data_directory, \"train.json\"), \"r\") as f:\n",
    "        train = json.load(f)\n",
    "\n",
    "    with open(os.path.join(data_directory, \"validation.json\"), \"r\") as f:\n",
    "        valid = json.load(f)\n",
    "\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "class NLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict: dict):\n",
    "        self.data_dict = data_dict\n",
    "        dd = data_dict\n",
    "\n",
    "        if len(dd[\"premise\"]) != len(dd[\"hypothesis\"]) or len(dd[\"premise\"]) != len(\n",
    "                dd[\"label\"]\n",
    "        ):\n",
    "            raise AttributeError(\"Incorrect length in data_dict\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict[\"premise\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dd = self.data_dict\n",
    "        return dd[\"premise\"][idx], dd[\"hypothesis\"][idx], dd[\"label\"][idx]\n",
    "\n",
    "\n",
    "def train_distilbert(model, loader, device, optimizer):\n",
    "    model.train()\n",
    "    criterion = model.get_criterion()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for premise, hypothesis, target in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = model.tokenize(premise, hypothesis).to(device)\n",
    "        target = target.to(device, dtype=torch.float32)\n",
    "\n",
    "        pred = model(inputs)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_distilbert(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    preds = []\n",
    "\n",
    "    for premise, hypothesis, target in loader:\n",
    "        preds.append(model(model.tokenize(premise, hypothesis).to(device)))\n",
    "\n",
    "        targets.append(target)\n",
    "\n",
    "    return torch.cat(preds), torch.cat(targets)\n",
    "\n",
    "\n",
    "# ######################## PART 1: YOUR WORK STARTS HERE ########################\n",
    "'''\n",
    "1. Finetune DistilBERT for classification (40 pts)\n",
    "In this part, you will use the NLI training data (same as A1) to finetune a DistilBERT model and\n",
    "predict whether a premise entails a hypothesis or not. Just like the first assignment, you will\n",
    "have to implement various parts of a custom nn.Module that loads a pretrained DistilBERT from\n",
    "Huggingface transformers. You can learn more about DistilBERT here, but you can just assume\n",
    "it’s a smaller version of BERT that remains fairly accurate.\n",
    "'''\n",
    "\n",
    "\n",
    "# You will have to implement the init function of the CustomDistilBert class. You will need to\n",
    "# initialize the following attributes:\n",
    "# ● self.distilbert\n",
    "# ● self.tokenizer\n",
    "# ● self.pred_layer\n",
    "# ● self.sigmoid\n",
    "# ● self.criterion\n",
    "# For distilbert and tokenizer, you will need to use transformers, whereas pred_layer, sigmoid,\n",
    "# and criterion require torch and correspond to questions you have previously answered in A1.\n",
    "\n",
    "\n",
    "class CustomDistilBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.__init__\n",
    "        Note:\n",
    "        ● Load the DistilBERT model's pretrained \"base uncased\" weights from the Huggingface\n",
    "        repository. We want the bare encoder outputting hidden-states without any specific head\n",
    "        on top.\n",
    "        ● Load the corresponding pre-trained tokenizer using the same method.\n",
    "        ● self.pred_layer takes the output of the model and predicts a single score (binary, 1 or 0),\n",
    "        then pass the output to the sigmoid layer\n",
    "        ● self.sigmoid should return torch's sigmoid activation.\n",
    "        ● self.criterion should be the binary cross-entropy loss. You may use torch.nn here.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: your work below\n",
    "        self.distilbert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.pred_layer = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    # vvvvv DO NOT CHANGE BELOW THIS LINE vvvvv\n",
    "    def get_distilbert(self):\n",
    "        return self.distilbert\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_pred_layer(self):\n",
    "        return self.pred_layer\n",
    "\n",
    "    def get_sigmoid(self):\n",
    "        return self.sigmoid\n",
    "\n",
    "    def get_criterion(self):\n",
    "        return self.criterion\n",
    "\n",
    "    # ^^^^^ DO NOT CHANGE ABOVE THIS LINE ^^^^^\n",
    "\n",
    "    def assign_optimizer(self, **kwargs):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.assign_optimizer\n",
    "        This assigns the Adam optimizer to this model's parameters (self) and returns the optimizer.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return torch.optim.Adam(self.parameters(), **kwargs)\n",
    "\n",
    "    def slice_cls_hidden_state(\n",
    "            self, x\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Edit the method CustomDistilBert.slice_cls_hidden_state. This is a helper method that will be\n",
    "        used inside forward, and will convert the output of your transformer model to something that can\n",
    "        be input in the prediction layer.\n",
    "        CustomDistilBert.slice_cls_hidden_state\n",
    "        Using the output of the model, return the last hidden state of the CLS token.\n",
    "        ParameterTypeDescription\n",
    "        xBaseModelOutputThe output of the distilbert model. You need to retrieve\n",
    "        the hidden state of the last output layer, then slice it to\n",
    "        obtain the hidden representation. The last hidden state\n",
    "        has shape: [batch_size, sequence_length,\n",
    "        hidden_size]\n",
    "        ReturnsDescription\n",
    "        Tensor[batch_size,\n",
    "        hidden_size]The last layer's hidden state representing the [CLS] token.\n",
    "        Usually, CLS is the first token in the sequence.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return x.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "    def tokenize(\n",
    "            self,\n",
    "            premise: \"list[str]\",\n",
    "            hypothesis: \"list[str]\",\n",
    "            max_length: int = 128,\n",
    "            truncation: bool = True,\n",
    "            padding: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use the get_tokenizer function implemented in 2.1 to write the method\n",
    "        CustomDistilBert.tokenize. That method is specifically to help you understand how the\n",
    "        tokenizer works, and should be fairly straightforward.\n",
    "        This function will be applied to the premise and hypothesis (list of str) to obtain the inputs for\n",
    "        your model. You will need to use the Huggingface tokenizer returned by get_tokenizer().\n",
    "        ParameterTypeDescription\n",
    "        premiselist of strThe first text to be input in your model.\n",
    "        hypothesislist of strThe second text to be input in your model.\n",
    "        For the remaining params, see documentations.\n",
    "        ReturnsDescription\n",
    "        BatchEncodingA dictionary-like object that can be given to the model (you\n",
    "        can find out how by reading the docs)\n",
    "        :param premise:\n",
    "        :param hypothesis:\n",
    "        :param max_length:\n",
    "        :param truncation:\n",
    "        :param padding:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return self.tokenizer(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            max_length=max_length,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Given the output of your tokenizer (a BatchEncoding object), you will have to pass through your\n",
    "        custom DistilBert model and output a score between 0 and 1 for each element in your batch;\n",
    "        this score represents whether there’s an entailment or not.\n",
    "        CustomDistilBert.forward\n",
    "        Note: In the original BERT paper, the output representation of CLS is used for classification.\n",
    "        You will need to slice the output of your DistilBERT to obtain the representation before giving it\n",
    "        to the last layer with sigmoid activation.\n",
    "        :param inputs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        x = self.distilbert(**inputs, return_dict=True)\n",
    "        x = self.slice_cls_hidden_state(x)\n",
    "        x = self.pred_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ######################## PART 2: YOUR WORK HERE ########################\n",
    "def freeze_params(model):\n",
    "    \"\"\"\n",
    "    Before starting, you will need to freeze all the parameters (including the embedding!). This is\n",
    "    because prompt tuning relies on tuning a very small number of fixed parameters (aka “prompts”,\n",
    "    since they are inserted as input embeddings to the model). Thus, everything else, including the\n",
    "    input word embeddings, are not trainable.\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO: your work below\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def pad_attention_mask(mask, p):\n",
    "    \"\"\"\n",
    "    Pad the start of the sequence p times of the attention_mask (which is one of the various\n",
    "    outputs of a Huggingface tokenizer) because the sequence length has changed. Find the\n",
    "    correct value based on Huggingface documentations.\n",
    "    :param mask:\n",
    "    :param p:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO: your work below\n",
    "    return F.pad(mask, (p, 0), value=1)\n",
    "\n",
    "\n",
    "class SoftPrompting(nn.Module):\n",
    "    def __init__(self, p: int, e: int):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.e = e\n",
    "\n",
    "        self.prompts = torch.randn((p, e), requires_grad=True)\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        \"\"\"\n",
    "        This takes the output of model.embeddings and adds the soft prompts, as described in the\n",
    "        paper. The prompts must be added at the start of the sequence.\n",
    "        ParameterTypeDescription\n",
    "        embeddedTensor[B, L, E]This corresponds to model.embeddings (where model is\n",
    "        a Huggingface transformer)\n",
    "        ● B: Batch size\n",
    "        ● L: Sequence Length\n",
    "        ● E: Embedding dimension (same as e)ReturnsDescription\n",
    "        Tensor[B, L+p, E]The input_embed to be given to the model, but with the added\n",
    "        :param embedded:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        p = self.prompts.unsqueeze(0).repeat(embedded.size(0), 1, 1).to(embedded.device)\n",
    "        return torch.cat([p, embedded], dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.', 'Two young boys of opposing teams play football, while wearing full protection uniforms and helmets.', 'A man in a blue shirt standing in front of a garage-like structure painted with geometric designs.'], 'hypothesis': ['Two kids in numbered jerseys wash their hands.', 'boys play football', 'A man is wearing a black shirt'], 'label': [0, 0, 1]}\n",
      "{'premise': ['Two brown dogs barking at each other.', 'four friends cheerfully jumping off the flight stairs.', 'Two construction workers are working on a scaffold.'], 'hypothesis': ['The animals are making noise.', 'Four people jumping off stairs.', 'The two workers are working on a car.'], 'label': [0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "# get data of validation set of index 2, 6, 9\n",
    "train_raw, valid_raw = load_datasets(\"data/nli\")\n",
    "valid_raw = {k: [v[i] for i in [2, 6, 9]] for k, v in valid_raw.items()}\n",
    "# print them\n",
    "print(valid_raw)\n",
    "# get from train set\n",
    "train_raw = {k: [v[i] for i in [2, 6, 9]] for k, v in train_raw.items()}\n",
    "# print them\n",
    "print(train_raw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.__init__\n",
    "        Note:\n",
    "        ● Load the DistilBERT model's pretrained \"base uncased\" weights from the Huggingface\n",
    "        repository. We want the bare encoder outputting hidden-states without any specific head\n",
    "        on top.\n",
    "        ● Load the corresponding pre-trained tokenizer using the same method.\n",
    "        ● self.pred_layer takes the output of the model and predicts a single score (binary, 1 or 0),\n",
    "        then pass the output to the sigmoid layer\n",
    "        ● self.sigmoid should return torch's sigmoid activation.\n",
    "        ● self.criterion should be the binary cross-entropy loss. You may use torch.nn here.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: your work below\n",
    "        self.distilbert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pred_layer = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    # vvvvv DO NOT CHANGE BELOW THIS LINE vvvvv\n",
    "    def get_distilbert(self):\n",
    "        return self.distilbert\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_pred_layer(self):\n",
    "        return self.pred_layer\n",
    "\n",
    "    def get_sigmoid(self):\n",
    "        return self.sigmoid\n",
    "\n",
    "    def get_criterion(self):\n",
    "        return self.criterion\n",
    "\n",
    "    # ^^^^^ DO NOT CHANGE ABOVE THIS LINE ^^^^^\n",
    "\n",
    "    def assign_optimizer(self, **kwargs):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.assign_optimizer\n",
    "        This assigns the Adam optimizer to this model's parameters (self) and returns the optimizer.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return torch.optim.Adam(self.parameters(), **kwargs)\n",
    "\n",
    "    def slice_cls_hidden_state(\n",
    "            self, x\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Edit the method CustomDistilBert.slice_cls_hidden_state. This is a helper method that will be\n",
    "        used inside forward, and will convert the output of your transformer model to something that can\n",
    "        be input in the prediction layer.\n",
    "        CustomDistilBert.slice_cls_hidden_state\n",
    "        Using the output of the model, return the last hidden state of the CLS token.\n",
    "        ParameterTypeDescription\n",
    "        xBaseModelOutputThe output of the distilbert model. You need to retrieve\n",
    "        the hidden state of the last output layer, then slice it to\n",
    "        obtain the hidden representation. The last hidden state\n",
    "        has shape: [batch_size, sequence_length,\n",
    "        hidden_size]\n",
    "        ReturnsDescription\n",
    "        Tensor[batch_size,\n",
    "        hidden_size]The last layer's hidden state representing the [CLS] token.\n",
    "        Usually, CLS is the first token in the sequence.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return x.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "    def tokenize(\n",
    "            self,\n",
    "            premise: \"list[str]\",\n",
    "            hypothesis: \"list[str]\",\n",
    "            max_length: int = 128,\n",
    "            truncation: bool = True,\n",
    "            padding: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use the get_tokenizer function implemented in 2.1 to write the method\n",
    "        CustomDistilBert.tokenize. That method is specifically to help you understand how the\n",
    "        tokenizer works, and should be fairly straightforward.\n",
    "        This function will be applied to the premise and hypothesis (list of str) to obtain the inputs for\n",
    "        your model. You will need to use the Huggingface tokenizer returned by get_tokenizer().\n",
    "        ParameterTypeDescription\n",
    "        premiselist of strThe first text to be input in your model.\n",
    "        hypothesislist of strThe second text to be input in your model.\n",
    "        For the remaining params, see documentations.\n",
    "        ReturnsDescription\n",
    "        BatchEncodingA dictionary-like object that can be given to the model (you\n",
    "        can find out how by reading the docs)\n",
    "        :param premise:\n",
    "        :param hypothesis:\n",
    "        :param max_length:\n",
    "        :param truncation:\n",
    "        :param padding:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return self.tokenizer(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            max_length=max_length,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Given the output of your tokenizer (a BatchEncoding object), you will have to pass through your\n",
    "        custom DistilBert model and output a score between 0 and 1 for each element in your batch;\n",
    "        this score represents whether there’s an entailment or not.\n",
    "        CustomDistilBert.forward\n",
    "        Note: In the original BERT paper, the output representation of CLS is used for classification.\n",
    "        You will need to slice the output of your DistilBERT to obtain the representation before giving it\n",
    "        to the last layer with sigmoid activation.\n",
    "        :param inputs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        x = self.distilbert(**inputs, return_dict=True)\n",
    "        x = self.slice_cls_hidden_state(x)\n",
    "        x = self.pred_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CustomRobertaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.__init__\n",
    "        Note:\n",
    "        ● Load the DistilBERT model's pretrained \"base uncased\" weights from the Huggingface\n",
    "        repository. We want the bare encoder outputting hidden-states without any specific head\n",
    "        on top.\n",
    "        ● Load the corresponding pre-trained tokenizer using the same method.\n",
    "        ● self.pred_layer takes the output of the model and predicts a single score (binary, 1 or 0),\n",
    "        then pass the output to the sigmoid layer\n",
    "        ● self.sigmoid should return torch's sigmoid activation.\n",
    "        ● self.criterion should be the binary cross-entropy loss. You may use torch.nn here.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: your work below\n",
    "        self.distilbert = transformers.RobertaModel.from_pretrained('roberta-base')\n",
    "        self.tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.pred_layer = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    # vvvvv DO NOT CHANGE BELOW THIS LINE vvvvv\n",
    "    def get_distilbert(self):\n",
    "        return self.distilbert\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_pred_layer(self):\n",
    "        return self.pred_layer\n",
    "\n",
    "    def get_sigmoid(self):\n",
    "        return self.sigmoid\n",
    "\n",
    "    def get_criterion(self):\n",
    "        return self.criterion\n",
    "\n",
    "    # ^^^^^ DO NOT CHANGE ABOVE THIS LINE ^^^^^\n",
    "\n",
    "    def assign_optimizer(self, **kwargs):\n",
    "        \"\"\"\n",
    "        CustomDistilBert.assign_optimizer\n",
    "        This assigns the Adam optimizer to this model's parameters (self) and returns the optimizer.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return torch.optim.Adam(self.parameters(), **kwargs)\n",
    "\n",
    "    def slice_cls_hidden_state(\n",
    "            self, x\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Edit the method CustomDistilBert.slice_cls_hidden_state. This is a helper method that will be\n",
    "        used inside forward, and will convert the output of your transformer model to something that can\n",
    "        be input in the prediction layer.\n",
    "        CustomDistilBert.slice_cls_hidden_state\n",
    "        Using the output of the model, return the last hidden state of the CLS token.\n",
    "        ParameterTypeDescription\n",
    "        xBaseModelOutputThe output of the distilbert model. You need to retrieve\n",
    "        the hidden state of the last output layer, then slice it to\n",
    "        obtain the hidden representation. The last hidden state\n",
    "        has shape: [batch_size, sequence_length,\n",
    "        hidden_size]\n",
    "        ReturnsDescription\n",
    "        Tensor[batch_size,\n",
    "        hidden_size]The last layer's hidden state representing the [CLS] token.\n",
    "        Usually, CLS is the first token in the sequence.\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return x.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "    def tokenize(\n",
    "            self,\n",
    "            premise: \"list[str]\",\n",
    "            hypothesis: \"list[str]\",\n",
    "            max_length: int = 128,\n",
    "            truncation: bool = True,\n",
    "            padding: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use the get_tokenizer function implemented in 2.1 to write the method\n",
    "        CustomDistilBert.tokenize. That method is specifically to help you understand how the\n",
    "        tokenizer works, and should be fairly straightforward.\n",
    "        This function will be applied to the premise and hypothesis (list of str) to obtain the inputs for\n",
    "        your model. You will need to use the Huggingface tokenizer returned by get_tokenizer().\n",
    "        ParameterTypeDescription\n",
    "        premiselist of strThe first text to be input in your model.\n",
    "        hypothesislist of strThe second text to be input in your model.\n",
    "        For the remaining params, see documentations.\n",
    "        ReturnsDescription\n",
    "        BatchEncodingA dictionary-like object that can be given to the model (you\n",
    "        can find out how by reading the docs)\n",
    "        :param premise:\n",
    "        :param hypothesis:\n",
    "        :param max_length:\n",
    "        :param truncation:\n",
    "        :param padding:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        return self.tokenizer(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            max_length=max_length,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Given the output of your tokenizer (a BatchEncoding object), you will have to pass through your\n",
    "        custom DistilBert model and output a score between 0 and 1 for each element in your batch;\n",
    "        this score represents whether there’s an entailment or not.\n",
    "        CustomDistilBert.forward\n",
    "        Note: In the original BERT paper, the output representation of CLS is used for classification.\n",
    "        You will need to slice the output of your DistilBERT to obtain the representation before giving it\n",
    "        to the last layer with sigmoid activation.\n",
    "        :param inputs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: your work below\n",
    "        x = self.distilbert(**inputs, return_dict=True)\n",
    "        x = self.slice_cls_hidden_state(x)\n",
    "        x = self.pred_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running test code for part 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 1.15MB/s]\n",
      "Downloading: 100%|██████████| 501M/501M [00:19<00:00, 25.5MB/s] \n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 14.2MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 9.89MB/s]\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'SoftPrompting' object has no attribute 'get_criterion'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleAttributeError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-7e0b644025bb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[0mvalid_losses3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_distilbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m     \u001B[0mloss2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_distilbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m     \u001B[0mloss3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_distilbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-630cc73c2836>\u001B[0m in \u001B[0;36mtrain_distilbert\u001B[0;34m(model, loader, device, optimizer)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtrain_distilbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m     \u001B[0mcriterion\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_criterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m     \u001B[0mtotal_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/grad_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    777\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    778\u001B[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001B[0;32m--> 779\u001B[0;31m             type(self).__name__, name))\n\u001B[0m\u001B[1;32m    780\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    781\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Module'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleAttributeError\u001B[0m: 'SoftPrompting' object has no attribute 'get_criterion'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score  # Make sure sklearn is installed\n",
    "\n",
    "random.seed(2022)\n",
    "torch.manual_seed(2022)\n",
    "\n",
    "# Parameters (you can change them)\n",
    "sample_size = 2500  # Change this if you want to take a subset of data for testing\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "num_words = 50000\n",
    "\n",
    "# If you use GPUs, use the code below:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ###################### PART 1: TEST CODE ######################\n",
    "# Prefilled code showing you how to use the helper functions\n",
    "train_raw, valid_raw = load_datasets(\"data/nli\")\n",
    "if sample_size is not None:\n",
    "    for key in [\"premise\", \"hypothesis\", \"label\"]:\n",
    "        train_raw[key] = train_raw[key][:sample_size]\n",
    "        valid_raw[key] = valid_raw[key][:sample_size]\n",
    "\n",
    "full_text = (\n",
    "        train_raw[\"premise\"]\n",
    "        + train_raw[\"hypothesis\"]\n",
    "        + valid_raw[\"premise\"]\n",
    "        + valid_raw[\"hypothesis\"]\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Running test code for part 1\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    NLIDataset(train_raw), batch_size=batch_size, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    NLIDataset(valid_raw), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "model = CustomDistilBert().to(device)\n",
    "optimizer = model.assign_optimizer(lr=1e-4)\n",
    "\n",
    "# ###################### PART 2: TEST CODE ######################\n",
    "freeze_params(model.get_distilbert())  # Now, model should have no trainable parameters\n",
    "\n",
    "sp = SoftPrompting(p=5, e=model.get_distilbert().embeddings.word_embeddings.embedding_dim).to(device)\n",
    "batch = model.tokenize(\n",
    "    [\"This is a premise.\", \"This is another premise.\"],\n",
    "    [\"This is a hypothesis.\", \"This is another hypothesis.\"],\n",
    ").to(device)\n",
    "batch.input_embedded = sp(model.get_distilbert().embeddings(batch.input_ids))\n",
    "batch.attention_mask = pad_attention_mask(batch.attention_mask, 5)\n",
    "\n",
    "# Get other two models\n",
    "model2 = CustomBert().to(device)\n",
    "model3 = CustomRobertaModel().to(device)\n",
    "\n",
    "# Get optimizer for the other two models\n",
    "optimizer2 = model2.assign_optimizer(lr=1e-4)\n",
    "optimizer3 = model3.assign_optimizer(lr=1e-4)\n",
    "\n",
    "# Get soft prompting for the other two models\n",
    "sp2 = SoftPrompting(p=5, e=model2.get_distilbert().embeddings.word_embeddings.embedding_dim).to(device)\n",
    "sp3 = SoftPrompting(p=5, e=model3.get_distilbert().embeddings.word_embeddings.embedding_dim).to(device)\n",
    "\n",
    "# In this section, you implemented soft prompt tuning and froze the model.\n",
    "# Just like the previous question, train your model for 10 epochs using the same training loop,\n",
    "# but this time the model itself is frozen and only the soft prompts are updated.\n",
    "# You will see the results are different from full finetuning.\n",
    "\n",
    "# Using the prompt tuning method described in the paper, train DistilBERT\n",
    "# Then, upload a plot of the validation loss, and another of the validation accuracy over each of the 10 epochs you trained the model for.\n",
    "# The legend should show the name of each model, which should all have different colors.\n",
    "\n",
    "valid_accs = []\n",
    "valid_losses = []\n",
    "valid_accs2 = []\n",
    "valid_losses2 = []\n",
    "valid_accs3 = []\n",
    "valid_losses3 = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Train models with soft prompting\n",
    "    loss = train_distilbert(sp, train_loader, device, optimizer)\n",
    "    loss2 = train_distilbert(model2, train_loader, device, optimizer2)\n",
    "    loss3 = train_distilbert(model3, train_loader, device, optimizer3)\n",
    "    valid_preds, valid_targets = eval_distilbert(sp, valid_loader, device)\n",
    "    valid_preds = valid_preds.round()\n",
    "    valid_preds2, valid_targets2 = eval_distilbert(model2, valid_loader, device)\n",
    "    valid_preds2 = valid_preds2.round()\n",
    "    valid_preds3, valid_targets3 = eval_distilbert(model3, valid_loader, device)\n",
    "    valid_preds3 = valid_preds3.round()\n",
    "    valid_accs.append(accuracy_score(valid_targets.cpu(), valid_preds.cpu()))\n",
    "    valid_loss = sp.get_criterion()(valid_preds, valid_targets.to(device, dtype=torch.float32)).item()\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs2.append(accuracy_score(valid_targets2.cpu(), valid_preds2.cpu()))\n",
    "    valid_loss2 = model2.get_criterion()(valid_preds2, valid_targets2.to(device, dtype=torch.float32)).item()\n",
    "    valid_losses2.append(valid_loss2)\n",
    "    valid_accs3.append(accuracy_score(valid_targets3.cpu(), valid_preds3.cpu()))\n",
    "    valid_loss3 = model3.get_criterion()(valid_preds3, valid_targets3.to(device, dtype=torch.float32)).item()\n",
    "    valid_losses3.append(valid_loss3)\n",
    "    print(f\"Epoch {epoch + 1}: train loss {loss:.4f}, valid loss {valid_loss:.4f}, valid acc {valid_accs[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}: train loss {loss2:.4f}, valid loss {valid_loss2:.4f}, valid acc {valid_accs2[-1]:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}: train loss {loss3:.4f}, valid loss {valid_loss3:.4f}, valid acc {valid_accs3[-1]:.4f}\")\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(valid_losses, label='valid loss of distilbert')\n",
    "plt.plot(valid_losses2, label='valid loss of bert')\n",
    "plt.plot(valid_losses3, label='valid loss of roberta')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation loss')\n",
    "plt.title('Validation Loss of DistilBERT, BERT, and Roberta')\n",
    "plt.legend()\n",
    "plt.savefig('validation_loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(valid_accs, label='valid acc of distilbert')\n",
    "plt.plot(valid_accs2, label='valid acc of bert')\n",
    "plt.plot(valid_accs3, label='valid acc of roberta')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('Validation Accuracy of DistilBERT, BERT, and Roberta')\n",
    "plt.legend()\n",
    "plt.savefig('validation_accuracy.png')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
